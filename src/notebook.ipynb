{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EXPLAINABLE MACHINE LEARNING\n",
        "\n",
        "@Author Gabriel Schurr, Ilyesse Hettenbach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATAPATH = str(os.getenv(\"DATAPATH\"))\n",
        "P_LABELS = os.path.join(DATAPATH, \"images_labels.txt\")\n",
        "DATAPATH = os.path.join(DATAPATH, \"animals\")\n",
        "print(f'Path to images: {DATAPATH}')\n",
        "print(f'Path to labels: {P_LABELS}')\n",
        "\n",
        "# P_LABELS = \"D:\\\\Database\\\\animals\\\\original\\\\images_labels.txt\"\n",
        "# DATAPATH = \"D:\\\\Database\\\\animals\\\\original\\\\animals\"\n",
        "\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = 'cpu'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "with open(P_LABELS, 'r') as f:\n",
        "    for line in f:\n",
        "        image_path, label = line.strip().split(' ')\n",
        "        data.append({'image_path': image_path, 'label': label})\n",
        "LABELS = pd.DataFrame(data)\n",
        "LABELS.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABELS.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(10, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    random_index = random.randint(0, len(LABELS)-1)\n",
        "    img = Image.open(LABELS['image_path'][random_index])\n",
        "    label = LABELS['label'][random_index]\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(label)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='label', data=LABELS)\n",
        "plt.title('Class Distribution')\n",
        "plt.xticks(rotation=60)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=90):\n",
        "        super(CustomResNet18, self).__init__()\n",
        "        self.output = None\n",
        "        self.resnet = models.resnet18(weights='IMAGENET1K_V1') # 'IMAGENET1K_V1'\n",
        "        # for param in self.resnet.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.output = self.resnet(x)\n",
        "        return self.output\n",
        "\n",
        "model = CustomResNet18()\n",
        "model = model.to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root=DATAPATH, transform=transform)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.95), len(dataset)-int(len(dataset)*0.95)])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tqdm(total=num_epochs, desc='Training') as pbar:\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            # print(f'[{epoch+1} /{2}, {i+1}/{len(train_loader)}] Loss: {running_loss/(i+1):.3f}')\n",
        "            pbar.set_postfix({'batch': f'{i+1}/{len(train_loader)}', 'loss': f'{running_loss/(i+1):.3f}'})\n",
        "\n",
        "        pbar.update(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted==labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the {len(val_loader)} test images: {100*correct/total}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GRAD-CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "idx = random.randint(0, len(val_dataset)-1)\n",
        "# print(f'Index: {idx}')\n",
        "image = Image.open(LABELS['image_path'][idx])\n",
        "target = LABELS['label'][idx]\n",
        "\n",
        "target_layer = model.resnet.layer2\n",
        "\n",
        "image_tensor = transform(image).unsqueeze(0)\n",
        "pred = model(image_tensor).argmax().item()\n",
        "\n",
        "cam = GradCAM(model=model, target_layers=target_layer)\n",
        "grayscale_cam = cam(input_tensor=image_tensor, targets=None)\n",
        "grayscale_cam = grayscale_cam[0, :]\n",
        "grayscale_cam = cv2.resize(grayscale_cam, (224, 224))\n",
        "\n",
        "image = image_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "image = image - np.min(image)\n",
        "image = image / np.max(image)\n",
        "\n",
        "visualization = show_cam_on_image(image, grayscale_cam, use_rgb=True, image_weight=0.5)\n",
        "plt.imshow(visualization)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
